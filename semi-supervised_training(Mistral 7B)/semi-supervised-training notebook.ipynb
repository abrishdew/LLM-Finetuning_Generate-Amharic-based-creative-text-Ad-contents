{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"Loading accelerator for later use-case \"\"\"\n",
    "from accelerate import FullyShardedDataParallelPlugin, Accelerator\n",
    "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
    "fsdp_plugin = FullyShardedDataParallelPlugin(\n",
    "    state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    "    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    ")\n",
    "accelerator = Accelerator(fsdp_plugin=fsdp_plugin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Base Model\n",
    "Now, we load the Mistral 7B base model using 4-bit quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "base_model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Loading the dataset from hugging face repository\n",
    "1. train set: used for training the model\n",
    "2. validation: used for validation set\n",
    "3. test_dataset = used for testing the dataset and checking the perfomance\n",
    "the data splitted into 70% trainingset 30% for testing and validation and further the 30% splitted into 50% test_set and 50%  validation_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Dataset:\n",
      "{'text': ['ADVERTISMENT áŠ áŠá‹« áˆ™áˆ€áˆ˜á‹µ áŠ¨áá‰°áŠ› á‹¨á‰£áˆ…áˆ áˆ•áŠ­áˆáŠ“ áŠ¥áŠ“ á‹˜áˆ˜áŠ“á‹Š á‹¨á‹‹áŒáˆá‰µ áŠ áŒˆáˆáŒáˆŽá‰µ á‹¨áˆáŠ•áˆ°áŒ£á‰¸á‹ á‹¨á‰£áˆ…áˆ áˆ…áŠ­áˆáŠ“á‹Žá‰½ âž¢ áˆˆá‹áŒ­áŠ“ áˆˆá‹áˆµáŒ¥ áŠªáŠ•á‰³áˆ®á‰µ âž¢ áˆˆáˆ›á‹µá‹«á‰µ âž¢ áˆˆáˆ±áŠ³áˆ­ á‰ áˆ½á‰³ âž¢áˆˆáŒ‰á‰ á‰µ(áˆˆá‹ˆá á‰ áˆ½á‰³) âž¢áˆˆáŒ¨áŒŽáˆ« áˆ…áˆ˜áˆ âž¢áˆˆáˆµáˆá‰°á‹ˆáˆ²á‰¥ âž¢áˆˆá‹°áˆ áŒáŠá‰µ âž¢áˆˆáŠ áˆµáˆ á‹ˆá‹­áˆ áˆ³á‹­áŠáˆµ âž¢áˆˆáˆšáŒ¥áˆ á‰ áˆºá‰³ âž¢ áˆˆáŠ¥áˆªáˆ…áŠ“ á‰áˆ­áŒ¥áˆ›á‰µ âž¢áˆˆáˆ«áˆµ áˆ…áˆ˜áˆ (áˆ›á‹­áŒáˆªáŠ•) âž¢áˆˆá‰ºáŒ áŠ“ áˆˆáŒ­áˆ­á‰µ âž¢áˆˆá‰‹á‰á‰»áŠ“ áŽáˆ¨áŽáˆ­ âž¢áˆˆáŠ¥áŒ¢áŠ“ áˆˆáŠ¥á‰£áŒ­ âž¢áˆˆá‹ˆáŒˆá‰¥ áˆ…áˆ˜áˆ âž¢áˆˆáˆ˜áŠ«áŠ•áŠá‰µ áˆˆá‹ˆá‹µáˆ áˆˆáˆ´á‰µáˆ âž¢áˆˆáŒ†áˆ®áŠ“ áˆˆáŠ á‹­áŠ• áˆ…áˆ˜áˆ âž¢áˆˆáˆ†á‹µ áˆ…áˆ˜áˆ âž¢á‹˜áˆ˜áŠ“á‹Š á‹¨á‹‹áŒáˆá‰µ áŠ áŒˆáˆáŒáˆŽá‰µ á‰ á‰°á‰‹áˆ›á‰½áŠ• áŠ¥áŠ•áˆ°áŒ£áˆˆáŠ•á¢ ðŸ‘‰áŠ¨áŠ¢á‰µá‹®áŒµá‹« á‰£áˆ…áˆ‹á‹Š áˆ…áŠ­áˆáŠ“ áŠ á‹‹á‰‚á‹Žá‰½ áˆ›áˆ…á‰ áˆ­ á‰ á‹˜áˆ­á‰ áˆ…áŒ‹á‹Š á‹¨á‰£áˆ…áˆ áˆ…áŠ­áˆáŠ“ áá‰ƒá‹µ á‹«áˆˆáŠ• áŠáŠ•á¢ áŠ á‹µáˆ«áˆ»:áŠ á‹²áˆµ áŠ á‰ á‰£ áŠ á‹¨áˆ­ áŒ¤áŠ“ áˆµáˆáŠ­ á‰áŒ¥áˆ­ ðŸ“²0927506650 ðŸ“²0987133734 ðŸ“²0939605455 á‰´áˆŒáŒáˆ«áˆ á‰»áŠ“áˆ‹á‰½áŠ• ntvE5NmM0', 'áŠ¨áŠ á‹áŠ“áŠ áŒ¨á‹‹á‰³á‹Žá‰½ áŠ¥áŠ“ áˆ½áˆáˆ›á‰¶á‰½ áŒ‹áˆ­ á‰ á‹«áˆ‹á‰½áˆá‰ á‰µ áŠ¥á‹¨á‹°áˆ¨áˆµáŠ• áŠá‹á¡á¡ á‹•á‹µáˆ‰ áŠ á‹«áˆáˆáŒ£á‰½áˆá¤ áŠ áˆ¨áŠ•áŒ“á‹´á‹ á‹¨áˆ³á‹áˆªáŠ®áˆ áŠ á‹á‰¶á‰¥áˆ³á‰½áŠ• á‰£áˆˆá‰ á‰µ á‰ áŠ©áˆ áˆµá‰³áˆá‰ á‰°á‰€áˆ‹á‰€áˆ‰áŠ• á‰ áŠ á‰¥áˆ®áŠá‰µ á‰†áŠ•áŒ† áŒŠá‹œ áŠ¥áŠ“áˆ³áˆá!\\n\\n#Gursha\\n#SafaricomEthiopia #FurtherAheadTogether', 'á‹­áŠá‰ á‰¥! á‰´á‹² áŠ ááˆ® áŠ¨áŠ áˆœáˆªáŠ« áˆ˜áˆáŠ­á‰µ áˆˆáŠ¢á‰µá‹®áŒµá‹«á‹á‹«áŠ• áŠ áˆµá‰°áˆ‹áˆ‹áŠá¢ á‹¶áŠ­á‰°áˆ­ áŠ á‰¥á‹­áŠ• áˆ€áŒˆáˆ­ á‹áˆµáŒ¥ áˆ†áŠœ á‰£áˆˆáˆ˜á‹°áŒˆáŒ á‰ á‰°áˆ°áˆá‰¶áŠ›áˆá¢', 'á‰ áˆ³á‹áˆªáŠ®áˆ áˆá‰¹ á‹¨á‹µáˆáŒ½ á£ á‹¨á…áˆá áˆ˜áˆá‹•áŠ­á‰µ áŠ¥áŠ“ áˆáŒ£áŠ• 4G á‹¨áŠ¢áŠ•á‰°áˆ­áŠ”á‰µ á‹µáˆ…áˆ¨ áŠ­áá‹« áŒ¥á‰…áˆŽá‰½ á‹«áˆˆáˆƒáˆ³á‰¥ áŠ¥áŠ•áˆµáˆ« á¤ áŠ•áŒá‹³á‰½áŠ•áŠ• áŠ¥áŠ“áˆ³á‹µáŒ!\\n\\náˆˆáŠ áŒˆáˆáŒáˆŽá‰± áˆˆáˆ˜áˆ˜á‹áŒˆá‰¥ á‹¨áˆ³á‹áˆªáŠ®áˆáŠ• á‹µáˆ…áˆ¨ - áŒˆá… áŠ¥áŠ•áŒŽá‰¥áŠá£ á‹ˆá‹° 0700 755 755 or 0700 700 755 áŠ¥áŠ•á‹°á‹áˆ á‹ˆá‹­áˆ á‹ˆá‹° enterprisesupport@safaricom.et áŠ¢áˆœá‹­áˆ áŠ¥áŠ•áˆ‹áŠ­á¢\\n\\n#SafaricomBusiness #FurtherAheadTogether', 'News Alert â€¼ï¸ á‹¨áˆ€á‹‹áˆ³ áŠ¨á‰°áˆ› áŠ¥áŠ“ áŠ áŠ«á‰£á‰¢á‹‹ á€áŒ¥á‰³ á‰ áˆ˜áŠ¨áˆ‹áŠ¨á‹« áŠ¥áŠ“ á‰ áŒá‹°áˆ«áˆ á“áˆŠáˆµ áŠ¥á‹ áˆ¥áˆ­ áŠ¥áŠ•á‹°áˆ†áŠ á‰°á‹áˆµáŠ—áˆ á‹¨áˆšáˆ áŠ¨áˆ˜áˆ¸ á‰ áˆ˜áˆ°áˆ«áŒ¨á‰µ áˆ‹á‹­ á‹­áŒˆáŠ›áˆ á‹¨á‹šáˆ… á‹œáŠ“ áŠ¥á‹áŠá‰³ áˆˆáˆ›áŒ£áˆ«á‰µ áˆ™áŠ¨áˆ« áŠ á‹µáˆ­áŒˆáŠ“ á‹¨á‹°áˆ¨áˆµáŠ•á‰ á‰µ áŠ¨áŠ¥á‹áŠá‰µ á‹¨áˆ«á‰€ áˆ˜áˆ†áŠ‘áŠ• áˆˆáˆ˜áˆ¨á‹³á‰µ á‰½áˆˆáŠ“áˆá¢ á‰ á‹šáˆ… áŒ‰á‹³á‹­ áˆ‹á‹­ áˆ˜áˆ¨áŒƒá‹ áŠ áˆˆáŠ á‹¨áˆá‰µáˆ‰ áˆ˜áˆ¨áŒƒ á‰  áˆ‹á‹­ áˆ˜áˆ‹áŠ­ á‰µá‰½áˆ‹áˆ‹á‰½áˆá¢'], 'Human_Label': ['healthcare &pharma', 'Telecom', 'Media', 'Telecom', 'Media']}\n",
      "\n",
      "Validation Dataset:\n",
      "{'text': ['ADVERTISMENT áŠ áŠá‹« áˆ™áˆ€áˆ˜á‹µ áŠ¨áá‰°áŠ› á‹¨á‰£áˆ…áˆ áˆ•áŠ­áˆáŠ“ áŠ¥áŠ“ á‹˜áˆ˜áŠ“á‹Š á‹¨á‹‹áŒáˆá‰µ áŠ áŒˆáˆáŒáˆŽá‰µ á‹¨áˆáŠ•áˆ°áŒ£á‰¸á‹ á‹¨á‰£áˆ…áˆ áˆ…áŠ­áˆáŠ“á‹Žá‰½ âž¢ áˆˆá‹áŒ­áŠ“ áˆˆá‹áˆµáŒ¥ áŠªáŠ•á‰³áˆ®á‰µ âž¢ áˆˆáˆ›á‹µá‹«á‰µ âž¢ áˆˆáˆ±áŠ³áˆ­ á‰ áˆ½á‰³ âž¢áˆˆáŒ‰á‰ á‰µ(áˆˆá‹ˆá á‰ áˆ½á‰³) âž¢áˆˆáŒ¨áŒŽáˆ« áˆ…áˆ˜áˆ âž¢áˆˆáˆµáˆá‰°á‹ˆáˆ²á‰¥ âž¢áˆˆá‹°áˆ áŒáŠá‰µ âž¢áˆˆáŠ áˆµáˆ á‹ˆá‹­áˆ áˆ³á‹­áŠáˆµ âž¢áˆˆáˆšáŒ¥áˆ á‰ áˆºá‰³ âž¢ áˆˆáŠ¥áˆªáˆ…áŠ“ á‰áˆ­áŒ¥áˆ›á‰µ âž¢áˆˆáˆ«áˆµ áˆ…áˆ˜áˆ (áˆ›á‹­áŒáˆªáŠ•) âž¢áˆˆá‰ºáŒ áŠ“ áˆˆáŒ­áˆ­á‰µ âž¢áˆˆá‰‹á‰á‰»áŠ“ áŽáˆ¨áŽáˆ­ âž¢áˆˆáŠ¥áŒ¢áŠ“ áˆˆáŠ¥á‰£áŒ­ âž¢áˆˆá‹ˆáŒˆá‰¥ áˆ…áˆ˜áˆ âž¢áˆˆáˆ˜áŠ«áŠ•áŠá‰µ áˆˆá‹ˆá‹µáˆ áˆˆáˆ´á‰µáˆ âž¢áˆˆáŒ†áˆ®áŠ“ áˆˆáŠ á‹­áŠ• áˆ…áˆ˜áˆ âž¢áˆˆáˆ†á‹µ áˆ…áˆ˜áˆ âž¢á‹˜áˆ˜áŠ“á‹Š á‹¨á‹‹áŒáˆá‰µ áŠ áŒˆáˆáŒáˆŽá‰µ á‰ á‰°á‰‹áˆ›á‰½áŠ• áŠ¥áŠ•áˆ°áŒ£áˆˆáŠ•á¢ ðŸ‘‰áŠ¨áŠ¢á‰µá‹®áŒµá‹« á‰£áˆ…áˆ‹á‹Š áˆ…áŠ­áˆáŠ“ áŠ á‹‹á‰‚á‹Žá‰½ áˆ›áˆ…á‰ áˆ­ á‰ á‹˜áˆ­á‰ áˆ…áŒ‹á‹Š á‹¨á‰£áˆ…áˆ áˆ…áŠ­áˆáŠ“ áá‰ƒá‹µ á‹«áˆˆáŠ• áŠáŠ•á¢ áŠ á‹µáˆ«áˆ»:áŠ á‹²áˆµ áŠ á‰ á‰£ áŠ á‹¨áˆ­ áŒ¤áŠ“ áˆµáˆáŠ­ á‰áŒ¥áˆ­ ðŸ“²0927506650 ðŸ“²0987133734 ðŸ“²0939605455 á‰´áˆŒáŒáˆ«áˆ á‰»áŠ“áˆ‹á‰½áŠ• ntvE5NmM0', 'ADVERTISMENT áˆ˜áˆáŠ«áˆ áŠáŒˆáˆ­ áŠ¨áˆ‰áˆ² á‹¨áŠ áŒ¥áŠ•á‰µáŠ“ á‹¨áˆ˜áŒˆáŒ£áŒ áˆšá‹« á‰€á‹¶ áˆ•áŠ­áˆáŠ“ áˆ›áŠ¥áŠ¨áˆ áˆˆáŠ áŒ¥áŠ•á‰µáŠ“ áˆˆáˆ˜áŒˆáŒ£áŒ áˆšá‹« á‰½áŒáˆ®á‰½áŠ“ áˆ…áˆ˜áˆžá‰½ áŠ á‹­áŠá‰°áŠ› áˆ˜áá‰µáˆ„ á‹¨áˆáŠ•áˆ°áŒ£á‰¸á‹ á‹¨áˆ…áŠ­áˆáŠ“ áŠ áŒˆáˆáŒáˆŽá‰¶á‰½ âœ”áˆ›áŠ•áŠ›á‹áˆ áˆµá‰¥áˆ«á‰µáŠ“ á‹áˆá‰ƒá‰µ áˆ•áŠ­áˆáŠ“ âœ”á‹¨áŒ‰áˆá‰ á‰µáŠ“ á‹³áˆŒ áˆ˜áŒˆáŒ£áŒ áˆšá‹« á‰½áŒáˆ­ âœ”á‹áˆµá‰¥áˆµá‰¥ áŠ¨á‰£á‹µ áˆµá‰¥áˆ«á‰¶á‰½áŠ“ á‹áˆá‰ƒá‰¶á‰½ âœ”á‹¨áŒ¡áŠ•á‰»áŠ“ á‹¨áŒ…áˆ›á‰µ áŒ‰á‹³á‰µ áˆ…áˆ˜áˆžá‰½áŠ• áˆ›áŠ¨áˆ âœ”á‹¨áˆ•áƒáŠ“á‰µ áŠ¥áŠ“ áŠ á‹‹á‰‚á‹Žá‰½ áŠ¥áŒ…áŠ“ áŠ¥áŒáˆ­ áˆ˜áŒ£áˆ˜áˆ áˆ›áˆµá‰°áŠ«áŠ¨áˆ âœ”áˆµá‰¥áˆ«á‰¶á‰½áŠ•áŠ“ á‹áˆá‰ƒá‰¶á‰½áŠ• á‹«áˆˆ á‰€á‹¶ áˆ•áŠ­áˆáŠ“ áˆ›áŠ¨áˆ âœ”á‰ á‰€á‹¶ áˆ•áŠ­áˆáŠ“ á‹¨áŒˆá‰¡ á‹¨á‰°áˆˆá‹«á‹© á‰¥áˆ¨á‰¶á‰½áŠ• áˆ›á‹áŒ£á‰µ âœ”á‹¨á•áˆ‹áˆµá‰²áŠ­ áˆ°áˆ­áŒ€áˆª áˆ•áŠ­áˆáŠ“ âœ”á‰ á‹³áˆŒáŠ“ á‹³áˆŒ áŒˆáŠ•á‹³ áˆµá‰¥áˆ«á‰¶á‰½áŠ• á‰ ááˆŽáˆ®áˆµáŠ®á’ á‹¨á‰³áŒˆá‹˜ áˆ•áŠ­áˆáŠ“ áˆ›á‹µáˆ¨áŒ ðŸ“Œáˆáˆá‹µáŠ“ á‰¥á‰ƒá‰µ á‰£áˆ‹á‰¸á‹ áˆµá”áˆ»áˆŠáˆµá‰µ áŠ¥áŠ“ áˆ°á‰¥ áˆµá”áˆ»áˆŠáˆµá‰µ áˆ€áŠªáˆžá‰½ áŠ¥áŒ…áŒ á‹˜áˆ˜áŠ“á‹Š á‰ áˆ†áŠ‘ á‹¨áˆ…áŠ­áˆáŠ“ áˆ˜áˆ³áˆªá‹«á‹ˆá‰½ á‰ áˆ˜á‰³áŒˆá‹ áˆáˆ‰áŠ•áˆ á‹¨áˆ…áŠ­áˆáŠ“ áŠ áŒˆáˆáŒáˆŽá‰¶á‰½ á‰ á‰°áˆ˜áŒ£áŒ£áŠ á‹‹áŒ‹ á‹«áŒˆáŠ›áˆ‰â€¼ áŠ á‹µáˆ«áˆ» : áŠ á‹¨áˆ­ áŒ¤áŠ“ áŒ…áˆ›á‰ áˆ­ á–áˆŠáˆµ áŒ£á‰¢á‹« áŒŽáŠ• ðŸ“² 0913468103 0953912229', 'ADVERTISMENT áˆ°áˆˆáˆžáŠ• áŒŒá‰³á‰¸á‹ á‹¨á‰°áˆ˜áˆ°áŠ¨áˆ¨áˆˆá‰µ á‹¨áˆ‚áˆ³á‰¥ áŠ á‹‹á‰‚áŠ“ á‹¨áŠ•áŒá‹µ áŠ áˆ›áŠ«áˆª á‹µáˆ­áŒ…á‰µ á‹¨áˆáŠ•áˆ°áŒ£á‰¸á‹ áŠ áŒˆáˆáŒáˆŽá‰¶á‰½ âœ” áˆˆá‹µáˆ­áŒ…á‰µá‹Ž á‰ IFRS á‹¨á‰³áŒˆá‹˜ á‹˜áˆ˜áŠ“á‹Š á‹¨áˆ‚áˆ³á‰¥ áŠ á‹«á‹«á‹ áˆµáˆ­áŠ£á‰µáŠ• áˆ˜á‹˜áˆ­áŒ‹á‰µ âœ”á‹¨áˆ‚áˆ³á‰¥ áˆ˜á‹áŒˆá‰¥ áŠ á‹«á‹«á‹ áˆµáˆáŒ áŠ“ áˆ˜áˆµáŒ á‰µ âœ” á‰ áˆ›áŠ•áŠ›á‹áˆ á‹¨á‰¢á‹áŠáˆµ áŒ‰á‹³á‹­ á‹¨áˆ›áˆ›áŠ¨áˆ­ áŠ áŒˆáˆáŒáˆŽá‰µ âœ”áˆˆáˆáˆ‰áˆ á‹˜áˆ­á á‹¨á•áˆ®áŒ€áŠ­á‰µ á‰¢á‹áŠáˆµ á•áˆ‹áŠ• áˆ›á‹˜áŒ‹áŒ€á‰µáŠ“ áˆµáˆˆ áˆµáˆ«á‹ á‰ á‹áˆ­á‹áˆ­ áˆ›áˆ›áŠ¨áˆ­ âœ” á‰ á’áŠ¤á‰½á‰µáˆª áŠ áŠ«á‹áŠ•á‰²áŒ áˆµáˆáŒ áŠ“ á‰ áŒáˆáˆ áˆ†áŠ á‰ á‰¡á‹µáŠ• áˆ˜áˆµáŒ á‰µ áŠ¨á‰¥á‹™ áŠ áŒˆáˆáŒáˆŽá‰¶á‰»á‰½áŠ• á‹­áŒ á‰€áˆ³áˆ‰á¢á‰ áŠ áŒˆáˆáŒáˆŽá‰¶á‰»á‰½áŠ• á‹­áˆ¨áŠ«áˆ‰á¢ áŠ á‹µáˆ«áˆ»:â€”á‹°áˆ´ áŠ¨á‰°áˆ› áˆáŠ•á‰µá‹‹á‰¥ áŠ á‹³áˆ«áˆ½ áŠá‰µ áˆˆáŠá‰µ áŠªáˆ«á‹­ á‰¤á‰¶á‰½ áˆ…áŠ•áƒ 2áŠ› áŽá‰… á‰¢áˆ® á‰áŒ¥áˆ­ S 6 áˆµáˆáŠ­:â€” ðŸ“² 0911839966 0913018399 0908001616 áˆ°áˆˆáˆžáŠ• áŒŒá‰³á‰¸á‹ á‹¨á‰°áˆ˜áˆ°áŠ¨áˆ¨áˆˆá‰µ á‹¨áˆ‚áˆ³á‰¥ áŠ á‹‹á‰‚áŠ“ á‹¨áŠ•áŒá‹µ áŠ áˆ›áŠ«áˆª á‹µáˆ­áŒ…á‰µ', 'Samsung A04 á‰ áˆ˜áŒá‹›á‰µ áˆ…á‹­á‹ˆá‰³á‰½áŠ•áŠ• áŠ¥áŠ“á‹˜áˆáŠ•! á‹›áˆ¬á‹áŠ‘ áŠ á‰…áˆ«á‰¢á‹«á‰½áŠ• á‹ˆá‹³áˆˆ á‹¨áˆ³á‹áˆªáŠ®áˆ áˆ±á‰… áŒŽáˆ« á‰ áˆ›áˆˆá‰µ á‹¨áŒáˆ‹á‰½áŠ• áŠ¥áŠ“á‹µáˆ­áŒˆá‹‰á¢\\n\\n#SafaricomEthiopia \\n#FurtherAheadTogether', 'áŠ¥áŠ•áŠ³áŠ• á‹°áˆµ áŠ áˆˆáˆ… á‹áˆ‚áˆ!\\xa0\\ná‹áˆ‚áˆ á‹šá‹«á‹µ áŠ¨á‹µáˆ¬á‹³á‹‹ á‰ áŠ¥áŒ£ á‰áŒ¥áˆ­ S026160222\\xa0 \\u200b\\u200bá‹¨á‰°áˆ¨áŠ­ á‰ áŒ‰áˆ­áˆ» áˆžá‰°áˆ­áˆ³á‹­áŠ­áˆ áŠ áˆ¸áŠ“áŠ áˆ†áŠ—áˆá¢\\n\\ná‹¨áˆ³á‹áˆªáŠ®áˆ áŠ áŒˆáˆáŒáˆŽá‰¶á‰½áŠ• á‰ áˆ˜áŒ á‰€áˆ á£ á‰€áˆªá‹Žá‰¹áŠ• áˆ½áˆáˆ›á‰¶á‰½ áŠ¥áŠ“áˆ¸áŠ•á!\\n\\n#FurtherAheadTogether'], 'Human_Label': ['healthcare &pharma', 'healthcare &pharma', 'financial service', 'Telecom', 'Telecom']}\n",
      "\n",
      "Test Dataset:\n",
      "{'text': ['ADVERTISMENT áŠ áŠá‹« áˆ™áˆ€áˆ˜á‹µ áŠ¨áá‰°áŠ› á‹¨á‰£áˆ…áˆ áˆ•áŠ­áˆáŠ“ áŠ¥áŠ“ á‹˜áˆ˜áŠ“á‹Š á‹¨á‹‹áŒáˆá‰µ áŠ áŒˆáˆáŒáˆŽá‰µ á‹¨áˆáŠ•áˆ°áŒ£á‰¸á‹ á‹¨á‰£áˆ…áˆ áˆ…áŠ­áˆáŠ“á‹Žá‰½ âž¢ áˆˆá‹áŒ­áŠ“ áˆˆá‹áˆµáŒ¥ áŠªáŠ•á‰³áˆ®á‰µ âž¢ áˆˆáˆ›á‹µá‹«á‰µ âž¢ áˆˆáˆ±áŠ³áˆ­ á‰ áˆ½á‰³ âž¢áˆˆáŒ‰á‰ á‰µ(áˆˆá‹ˆá á‰ áˆ½á‰³) âž¢áˆˆáŒ¨áŒŽáˆ« áˆ…áˆ˜áˆ âž¢áˆˆáˆµáˆá‰°á‹ˆáˆ²á‰¥ âž¢áˆˆá‹°áˆ áŒáŠá‰µ âž¢áˆˆáŠ áˆµáˆ á‹ˆá‹­áˆ áˆ³á‹­áŠáˆµ âž¢áˆˆáˆšáŒ¥áˆ á‰ áˆºá‰³ âž¢ áˆˆáŠ¥áˆªáˆ…áŠ“ á‰áˆ­áŒ¥áˆ›á‰µ âž¢áˆˆáˆ«áˆµ áˆ…áˆ˜áˆ (áˆ›á‹­áŒáˆªáŠ•) âž¢áˆˆá‰ºáŒ áŠ“ áˆˆáŒ­áˆ­á‰µ âž¢áˆˆá‰‹á‰á‰»áŠ“ áŽáˆ¨áŽáˆ­ âž¢áˆˆáŠ¥áŒ¢áŠ“ áˆˆáŠ¥á‰£áŒ­ âž¢áˆˆá‹ˆáŒˆá‰¥ áˆ…áˆ˜áˆ âž¢áˆˆáˆ˜áŠ«áŠ•áŠá‰µ áˆˆá‹ˆá‹µáˆ áˆˆáˆ´á‰µáˆ âž¢áˆˆáŒ†áˆ®áŠ“ áˆˆáŠ á‹­áŠ• áˆ…áˆ˜áˆ âž¢áˆˆáˆ†á‹µ áˆ…áˆ˜áˆ âž¢á‹˜áˆ˜áŠ“á‹Š á‹¨á‹‹áŒáˆá‰µ áŠ áŒˆáˆáŒáˆŽá‰µ á‰ á‰°á‰‹áˆ›á‰½áŠ• áŠ¥áŠ•áˆ°áŒ£áˆˆáŠ•á¢ ðŸ‘‰áŠ¨áŠ¢á‰µá‹®áŒµá‹« á‰£áˆ…áˆ‹á‹Š áˆ…áŠ­áˆáŠ“ áŠ á‹‹á‰‚á‹Žá‰½ áˆ›áˆ…á‰ áˆ­ á‰ á‹˜áˆ­á‰ áˆ…áŒ‹á‹Š á‹¨á‰£áˆ…áˆ áˆ…áŠ­áˆáŠ“ áá‰ƒá‹µ á‹«áˆˆáŠ• áŠáŠ•á¢ áŠ á‹µáˆ«áˆ»:áŠ á‹²áˆµ áŠ á‰ á‰£ áŠ á‹¨áˆ­ áŒ¤áŠ“ áˆµáˆáŠ­ á‰áŒ¥áˆ­ ðŸ“²0927506650 ðŸ“²0987133734 ðŸ“²0939605455 á‰´áˆŒáŒáˆ«áˆ á‰»áŠ“áˆ‹á‰½áŠ• ntvE5NmM0', 'Bulk SMS áˆˆáˆáˆ‰áˆ á‰¢á‹áŠáˆ¶á‰»á‰½áŠ• á‹¨áˆšáˆ˜á‰¹ áŒ¥á‰…áˆŽá‰½ á‹­á‹ž áˆ˜á‰·áˆá¢ áŠ¨áˆáˆˆá‰± áŠ áˆ›áˆ«áŒ®á‰½ á‰ áˆ˜áˆáˆ¨áŒ¥ á‹›áˆ¬á‹áŠ‘ áˆˆá‹°áŠ•á‰ áŠžá‰»á‰½áŠ• áŠ¥áŠ•á‹°á‹áˆá¤ á‰¢á‹áŠáˆ³á‰½áŠ•áŠ• áŠ¥áŠ“áŒ£áŒ¡áá¢\\n\\náˆˆáŠ áŒˆáˆáŒáˆŽá‰± áˆˆáˆ˜áˆ˜á‹áŒˆá‰¥ á‹µáˆ…áˆ¨ - áŒˆáŒ»á‰½áŠ•áŠ• á‹­áŒŽá‰¥áŠ™á£ á‹ˆá‹° 0700 755 755 or  0700 700 755 á‹­á‹°á‹áˆ‰ á‹ˆá‹­áˆ á‹ˆá‹° enterprisesupport@safaricom.et áŠ¢áˆœá‹­áˆ á‹­áˆ‹áŠ©\\n\\n#SafaricomBusiness #FurtherAheadTogether', 'â¬†ï¸â¬†ï¸áˆµáˆˆ á‹°á‰¥áˆ¨ áˆ›áˆ­á‰†áˆµ á‹©áŠ’á‰¨áˆ­áˆ²á‰² áŠ áŒ­áˆ­ áˆ˜áˆ¨áŒƒ! á‹©áŠ’á‰¨áˆ­áˆ²á‰²á‹ á‹áˆµáŒ¥ áŠ¨áˆšáŒˆáŠ™ áˆ°á‹Žá‰½ á‹«áŒˆáŠ˜áˆá‰µ áˆ˜áˆ¨áŒƒ: áŠ áŠ•á‹µ á‹¨á‹©áŠ’á‰¨áˆ­áˆ²á‰²á‹ á‹¨á‰°áˆ›áˆªá‹Žá‰½ áˆ…á‰¥áˆ¨á‰µ áŠ á‰£áˆ á‹¨áˆ†áŠ á‰°áˆ›áˆª á‰µáŠ“áŠ•á‰µ áŒ á‹‹á‰µ 4 áˆ°áŠ á‰µ áŠ áŠ«á‰£á‰¢ á‰ á‹°áˆ¨áˆ°á‰ á‰µ á‹µá‰¥á‹°á‰£ áˆ…á‹­á‹ˆá‰± áŠ áˆááˆá¢ á‹¨áŒáŒ­á‰± áˆ˜áŠáˆ» áŠ¥áˆµáŠ«áˆáŠ• áŠ áˆá‰³á‹ˆá‰€áˆá£ áŒáŠ• áˆŸá‰¹ áˆ²áˆ¨á‰¥áˆ¹ á‹¨áŠá‰ áˆ© á‰°áˆ›áˆªá‹Žá‰½áŠ• á‰ªá‹µá‹® áˆ²á‰€áˆ­á… áŠá‰ áˆ­ áˆ²á‰£áˆ áˆ°áˆá‰°áŠ“áˆá¢ áˆáŠ“áˆá‰£á‰µ á€á‰¡ á‰ á‹šáˆ… á‹¨á‰°áŠáˆ³ áˆŠáˆ†áŠ• á‹­á‰½áˆ‹áˆá¢ áŠ áˆáŠ• áˆ‹á‹­ á‰µáˆáˆ…áˆ­á‰µ áˆ™áˆ‰ áˆˆáˆ™áˆ‰ á‰°á‰‹áˆ­áŒ§áˆá¢ á‰µáŠ“áŠ•á‰µ áˆ…á‹­á‹ˆá‰± á‹«áˆˆáˆá‹ á‰°áˆ›áˆª á‹¨á‰µáŒáˆ«á‹­ á‰°á‹ˆáˆ‹áŒ… áŠá‹á¢ á‰ á‹šáˆ… áˆáŠ­áŠ•á‹«á‰µ á‰µáŠ“áŠ•á‰µ áˆáˆ½á‰µ á‹¨á‰µáŒáˆ«á‹­ á‰°áˆ›áˆªá‹Žá‰½ áŠ áŠ•á‹µ áŠ á‹³áˆ«áˆ½ á‹áˆµáŒ¥ áŠ á‹µáˆ¨á‹‹áˆá¢ á‹­áˆ…áˆ á‹°áˆ…áŠ•áŠá‰³á‰¸á‹áŠ• áˆˆáˆ˜áŒ á‰ á‰… á‰³áˆµá‰¦ á‹­áˆ˜áˆµáˆˆáŠ“áˆá¢ Via Elias Meseret', 'ADVERTISMENT á‹ˆáˆŽ áˆ€á‹­á‰… á‰¤á‰µ áŠ¨áˆáˆˆáŒ‰ á‹­á‹°á‹áˆ‰ ðŸ‘‡ á‹¶/áˆ­ áŠ á‰¥á‹±á£ á‹¶/áˆ­ áˆ™áˆ€áˆ˜á‹µ áŠ¥áŠ“ áŒ“á‹°áŠžá‰»á‰¸á‹ áˆªáˆáˆµá‰´á‰µ á‹¨áˆ›áˆáˆ›á‰µáŠ“ á‹¨áˆ›áŠ¨á‹áˆáˆ áˆ…/áˆ½/áˆ› á‰ á‹ˆáˆŽ áˆ€á‹­á‰… áŠ¨á‰°áˆ› á‹¨áŠ á–áˆ­á‰³áˆ› áŠ¥áŒ£ áˆ½á‹«áŒ­ áŒ€áˆáˆ¯áˆá¡á¡ â‘  á‹¨á‰¦á‰³á‹ áˆ˜áŒˆáŠ› áˆ€á‹­á‰… áŠ¨á‰°áˆ› áŠ¨á’á‹«áˆ³ 250áˆœ á‹ˆá‹° áˆ€á‹­á‰ áŠ á‰…áŒ£áŒ« áŠ á‹²áˆ± áŠ áˆµá“áˆá‰µ á‹³áˆ­ â‘¡ á‹¨á‰¦á‰³á‹ áŠ áŒˆáˆáŒáˆŽá‰µ á‰…á‹­áŒ¥ â‘¢ áŠ áŠ•á‹µ áŠ¥áŒ£ 60 áŠ«áˆœ á‹¨áˆ˜áŠ–áˆªá‹« áŠ á“áˆ­á‰³áˆ› áŠ¥áŠ“ 10 áŠ«áˆ¬ á‹¨áˆ±á‰… á‰¦á‰³ áŠ¨á“áˆ­áŠªáŠ•áŒ áŒ‹áˆ­ â‘£ áŒáŠ•á‰£á‰³ á‰ áŒ‹áˆ« á‹ˆáŒª áŠá‹á¡á¡ â‘¤ á‰¦á‰³á‹ á‰ áˆªáˆáˆµá‰´á‰± á‰£áˆˆá‰¤á‰µáŠá‰µ á‹¨á‰°áˆ˜á‹˜áŒˆá‰  áŠá‹á¢ áˆˆá‰ áˆˆáŒ  áˆ˜áˆ¨áŒƒ:â€” 0937411111 0938411111 á‰³áˆ›áŠáŠá‰µ áˆ˜áŒˆáˆˆáŒ«á‰½áŠ• áŠá‹á¢ á‹¶/áˆ­ áŠ á‰¥á‹±á£ á‹¶/áˆ­ áˆ™áˆ€áˆ˜á‹µ áŠ¥áŠ“ áŒ“á‹°áŠžá‰»á‰¸á‹ áˆªáˆáˆµá‰´á‰µ á‹¨áˆ›áˆáˆ›á‰µáŠ“ á‹¨áˆ›áŠ¨á‹áˆáˆ áˆ…/áˆ½/áˆ›', 'ADVERTISMENT áˆ°áˆ‹áˆðŸ‘‹ áŠ¥áŠ•áŠ³áŠ• á‹ˆá‹° Writerâ€™s Desk Consultancy á‰ áˆ°áˆ‹áˆ áˆ˜áŒ¡â€¼áˆµáˆ«á‰½áŠ• áˆ›áˆ›áŠ¨áˆ­ áŠá‹á¢áˆˆá‹áŒ¤á‰µ á‹¨áˆšá‹«á‰ á‰ƒ áˆáŠ­áˆ­ áŠ¥áŠ•áˆ°áŒ£áˆˆáŠ•â€¼ â‘  á‰ªá‹› áˆ›áˆ›áŠ¨áˆ­ áŠ áŒˆáˆáŒáˆŽá‰µ âœ”áŠ¥áŠ•á‹° áŠ«áŠ“á‹³á£ áŠ á‹áˆµá‰µáˆ«áˆŠá‹«á£ áŒƒá“áŠ•á£ á‰»á‹­áŠ“ áŠ¥áŠ“ áˆŒáˆŽá‰½ áˆ‹áˆ‰ á‰³á‹‹á‰‚ áˆ˜á‹³áˆ¨áˆ»á‹Žá‰½ á‹¨áˆµáˆ« áŠ¥áŠ“ á‹¨áŒ‰á‰¥áŠá‰µ á‰ªá‹› áŠ¥áŠ•á‹²á‹«áŒˆáŠ™ áˆ›áˆ›áŠ¨áˆ­ âœ”áŠ áˆµáˆáˆ‹áŒŠ áˆ°áŠá‹¶á‰½áŠ• áŠ¨áˆ›á‹˜áŒ‹áŒ€á‰µ áŒ€áˆáˆ® áˆˆáŠ¢áŠ•á‰°áˆ­á‰ªá‹ áŠ¥áˆµáŠ­áˆ›áˆ°áˆáŒ áŠ• áŠ¥áŠ“ áŠ¨áá‰°áŠ› á‹¨á‰ªá‹› áˆ›áˆ¨áŒ‹áŒˆáŒ« á‰°áˆ˜áŠ–á‰½áŠ• á‰°áŒ á‰…áˆž á‹°áŠ•á‰ áŠžá‰½áŠ• áˆ›á‹˜áŒ‹áŒ€á‰µ â‘¡á‰°áˆ›áˆªá‹Žá‰½ áˆ›áˆ›áŠ¨áˆ­ áŠ áŒˆáˆáŒáˆŽá‰µ âœ”á‰°áˆ›áˆªá‹Žá‰½ á‰°áˆµáˆ›áˆš á‹¨áˆ†áŠ á‹¨á‹áŒª áˆ€áŒˆáˆ­ áŠ®áˆŒáŒ†á‰½áŠ• áŠ¥áŠ•á‹²á‹«áŒˆáŠ™ áˆ›áˆ›áŠ­áˆ­ âœ”áŠ¥áŠ•á‹° IELTS áˆ‹áˆ‰ áˆá‰°áŠ“á‹Žá‰½ á‰°áˆ›áˆªá‹Žá‰½áŠ• á‰¥á‰ áˆ›á‹µáˆ¨áŒ áŠ¥áŠ“ á‰°áˆ›áˆªá‹Žá‰½áŠ• áˆˆá‰ªá‹› áŠ¢áŠ•á‰°áˆ­á‰ªá‹ á‹¨áˆ›á‹˜áŒ‹áŒ€á‰µ áˆµáˆ« áŠ¥áŠ•áˆ°áˆ«áˆˆáŠ•á¡á¡ â‘¢á‹¨á…áˆá áŠ áŒˆáˆáŒáˆŽá‰¶á‰½á¡ âœ”á‹­áˆ… áŠ¨áá‰°áŠ› á‹²áŒáˆª á‹«áˆ‹á‰¸á‹ áŠ¥áŠ“ áŠ¨á‰°áˆˆá‹«á‹© á‹²á“áˆ­á‰µáˆ˜áŠ•á‰µ á‹¨áˆ˜áŒ¡ áˆˆáŠ áŠ«á‹³áˆšáŠ­ á€áˆ€áŠá‹Žá‰½ áŠ¥áŠ“ á‹¨áˆ‹á‰€ á‰½áˆŽá‰³ áˆ‹áˆ‹á‰¸á‹ á‹¨áˆáŒ áˆ« á€áˆƒáŠá‹Žá‰½ áŠá‹á¡á¡ âœ”á‹µáˆ­áŒ…á‰³á‰½áŠ• á‰°áˆ›áˆªá‹Žá‰½áŠ•á£ áŠ©á‰£áŠ•á‹«á‹Žá‰½áŠ• á‹ˆá‹­áˆ áŒáˆˆáˆ°á‰¦á‰½áŠ• 24/7 áŠ¨áˆšáŒˆáŠ™ á‹¨áŠ áŠ«á‹³áˆšáŠ­ á€áˆƒáŠá‹Žá‰½ áŒ‹áˆ­ á‹«áŒˆáŠ“áŠ›áˆ:: áˆˆáŠ¥áˆ­áˆµá‹Ž á‹¨á‰²áˆ²áˆµ á‹ˆáˆ¨á‰€á‰¶á‰½á£ blog á…áˆáŽá‰½á£ website contentá£ á‹ˆá‹­áˆ áˆ›áŠ•áŠ›á‹áˆ á‹¨á…áˆá á•áˆ®áŒ€áŠ­á‰¶á‰½ áˆ‹á‹­áˆ á‹«áˆ›áŠ­áˆ©á‹Žá‰³áˆá¢ ðŸ‘‰áˆˆáˆ›áŠ•áŠ›á‹áˆ á‹¨á‰ªá‹› áŠ áŒˆáˆáŒáˆŽá‰¶á‰½ á‹¨áˆ›áˆ˜áˆáŠ¨á‰» áŠ­áá‹« áŠ áˆˆáˆ›áˆµáŠ­áˆáˆ‹á‰½áŠ•áˆ áˆá‹© á‹«áˆ¨áŒˆáŠ“áˆâ€¼ Writerâ€™s Desk Consultancy á‹›áˆ¬á‹áŠ‘ á‹«áŒáŠ™áŠ•ðŸ‘‡ áˆµáˆáŠ­ á‰áŒ¥áˆ­á¡â€”0944096500 á‰´áˆŒáŒáˆ«áˆ á‰»áŠ“áˆ: áˆµáˆ«á‰½áŠ• áˆ›áˆ›áŠ¨áˆ­ áŠá‹á¢áˆˆá‹áŒ¤á‰µ á‹¨áˆšá‹«á‰ á‰ƒ áˆáŠ­áˆ­ áŠ¥áŠ•áˆ°áŒ£áˆˆáŠ•â€¼'], 'Human_Label': ['healthcare &pharma', 'Telecom', 'Media', 'financial service', 'financial service']}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the 'Tvsybkzkmapab/Amharic_ad_generation' dataset\n",
    "dataset = load_dataset('Tvsybkzkmapab/Amharic_ad_generation')\n",
    "\n",
    "# Access the 'train', 'validation', and 'test' splits\n",
    "train_dataset = dataset['train']\n",
    "validation_dataset = dataset['validation']\n",
    "test_dataset = dataset['test']\n",
    "\n",
    "# Print the first few samples from each split\n",
    "print(\"\\nTraining Dataset:\")\n",
    "print(train_dataset[:5])\n",
    "\n",
    "print(\"\\nValidation Dataset:\")\n",
    "print(validation_dataset[:5])\n",
    "\n",
    "print(\"\\nTest Dataset:\")\n",
    "print(test_dataset[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenization steps using pretrained tokenizer from huggingface \n",
    "(iocuydi/llama-2-amharic-3784m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"iocuydi/llama-2-amharic-3784m\")  # Load your trained tokenizer\n",
    "\n",
    "tokenizer.model_max_length = 512  # Set maximum sequence length\n",
    "tokenizer.padding_side = \"left\"  # Set padding strategy\n",
    "tokenizer.add_eos_token = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(prompt):\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instruction to the model and the description of the attributes for the model\n",
    "to have context and instruct it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_tokenize_prompt(data_point):\n",
    "    text = data_point[\"text\"]\n",
    "    labels = data_point[\"Human_Label\"]\n",
    "\n",
    "    # Generate prompt template\n",
    "    template = f\"\"\"Given an Amharic advertisement sentence:\n",
    "\n",
    "\"{text}\"\n",
    "\n",
    "What are the main categories (tags) associated with this advertisement?\n",
    "\n",
    "Possible categories are: ['healthcare &pharma', 'Telecom', 'Media', 'financial service', 'consumer products', 'computing device', 'Realstate', 'retail', 'training', 'Entertainment', 'software development', 'Other']\n",
    "\n",
    "Target categories: \"{labels}\"\n",
    "\"\"\"\n",
    "\n",
    "    # Tokenize the prompt\n",
    "    return tokenize(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt)\n",
    "tokenized_val_dataset = validation_dataset.map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Up LoRA\n",
    "Now, we prepare the model for fine-tuning by applying LoRA adapters to the linear layers of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,  # Conventional\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)\n",
    "# Apply the accelerator. You can comment this out to remove the accelerator.\n",
    "model = accelerator.prepare_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Training\n",
    "In this step, we start training the fine-tuned model. You can adjust the training parameters according to your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() > 1: # If more than 1 GPU\n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from datetime import datetime\n",
    "import sentencepiece as spm\n",
    "\n",
    "\n",
    "\n",
    "project = \"Mistral_Amharic-finetuning\"\n",
    "base_model_name = \"mistral\"\n",
    "run_name = base_model_name + \"-\" + project\n",
    "output_dir = \"./\" + run_name\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    args=transformers.TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        warmup_steps=5,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        max_steps=1000,\n",
    "        learning_rate=2.5e-5, # Want about 10x smaller than the Mistral learning rate\n",
    "        logging_steps=50,\n",
    "        bf16=True,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        logging_dir=\"./logs\",        # Directory for storing logs\n",
    "        save_strategy=\"steps\",       # Save the model checkpoint every logging step\n",
    "        save_steps=50,                # Save checkpoints every 50 steps\n",
    "        evaluation_strategy=\"steps\", # Evaluate the model every logging step\n",
    "        eval_steps=50,               # Evaluate and save checkpoints every 50 steps\n",
    "        do_eval=True,                # Perform evaluation at the end of training\n",
    "        report_to=\"wandb\",           # Comment this out if you don't want to use weights & baises\n",
    "        run_name=f\"{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\"          # Name of the W&B run (optional)\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
