{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"Loading accelerator for later use-case \"\"\"\n",
    "from accelerate import FullyShardedDataParallelPlugin, Accelerator\n",
    "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
    "fsdp_plugin = FullyShardedDataParallelPlugin(\n",
    "    state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    "    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    ")\n",
    "accelerator = Accelerator(fsdp_plugin=fsdp_plugin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Base Model\n",
    "Now, we load the Mistral 7B base model using 4-bit quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "base_model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Loading the dataset from hugging face repository\n",
    "1. train set: used for training the model\n",
    "2. validation: used for validation set\n",
    "3. test_dataset = used for testing the dataset and checking the perfomance\n",
    "the data splitted into 70% trainingset 30% for testing and validation and further the 30% splitted into 50% test_set and 50%  validation_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Dataset:\n",
      "{'text': ['ADVERTISMENT ріарЇірІФ рѕЎрѕђрѕўрІх ріерЇЇрЅ░ріЏ рІерЅБрѕЁрѕЇ рѕЋріГрѕЮріЊ ріЦріЊ рІўрѕўріЊрІі рІерІІрїЇрѕЮрЅх ріарїѕрѕЇрїЇрѕјрЅх рІерѕЮріЋрѕ░рїБрЅИрІЇ рІерЅБрѕЁрѕЇ рѕЁріГрѕЮріЊрІјрЅй Ръб рѕѕрІЇрїГріЊ рѕѕрІЇрѕхрїЦ ріфріЋрЅ│рѕ«рЅх Ръб рѕѕрѕЏрІхрІФрЅх Ръб рѕѕрѕ▒рі│рѕГ рЅарѕйрЅ│ РъбрѕѕрїЅрЅарЅх(рѕѕрІѕрЇЇ рЅарѕйрЅ│) РъбрѕѕрїерїјрѕФ рѕЁрѕўрѕЮ РъбрѕѕрѕхрЇѕрЅ░рІѕрѕ▓рЅЦ РъбрѕѕрІ░рѕЮ рїЇрЇірЅх РъбрѕѕріарѕхрѕЮ рІѕрІГрѕЮ рѕ│рІГріљрѕх РъбрѕѕрѕџрїЦрѕЇ рЅарѕ║рЅ│ Ръб рѕѕріЦрѕфрѕЁріЊ рЅЂрѕГрїЦрѕЏрЅх РъбрѕѕрѕФрѕх рѕЁрѕўрѕЮ (рѕЏрІГрїЇрѕфріЋ) РъбрѕѕрЅ║рЇї ріЊ рѕѕрїГрѕГрЅх РъбрѕѕрЅІрЅЂрЅ╗ріЊ рЇјрѕерЇјрѕГ РъбрѕѕріЦрїбріЊ рѕѕріЦрЅБрїГ РъбрѕѕрІѕрїѕрЅЦ рѕЁрѕўрѕЮ РъбрѕѕрѕўріФріЋріљрЅх рѕѕрІѕрІхрѕЮ рѕѕрѕ┤рЅхрѕЮ Ръбрѕѕрїєрѕ«ріЊ рѕѕріарІГріЋ рѕЁрѕўрѕЮ РъбрѕѕрѕєрІх рѕЁрѕўрѕЮ РъбрІўрѕўріЊрІі рІерІІрїЇрѕЮрЅх ріарїѕрѕЇрїЇрѕјрЅх рЅарЅ░рЅІрѕЏрЅйріЋ ріЦріЋрѕ░рїБрѕѕріЋрЇб ­ЪЉЅріерібрЅхрІ«рїхрІФ рЅБрѕЁрѕІрІі рѕЁріГрѕЮріЊ ріарІІрЅѓрІјрЅй рѕЏрѕЁрЅарѕГ рЅарІўрѕГрЇЅ рѕЁрїІрІі рІерЅБрѕЁрѕЇ рѕЁріГрѕЮріЊ рЇЇрЅЃрІх рІФрѕѕріЋ ріљріЋрЇб ріарІхрѕФрѕ╗:ріарІ▓рѕх ріарЅарЅБ ріарІерѕГ рїцріЊ рѕхрѕЇріГ рЅЂрїЦрѕГ ­ЪЊ▓0927506650 ­ЪЊ▓0987133734 ­ЪЊ▓0939605455 рЅ┤рѕїрїЇрѕФрѕЮ рЅ╗ріЊрѕІрЅйріЋ ntvE5NmM0', 'ріеріарІЮріЊріЮ рїерІІрЅ│рІјрЅй ріЦріЊ рѕйрѕЇрѕЏрЅХрЅй рїІрѕГ рЅарІФрѕІрЅйрѕЂрЅарЅх ріЦрІерІ░рѕерѕхріЋ ріљрІЇрЇАрЇА рІЋрІхрѕЅ ріарІФрѕЮрѕЇрїБрЅйрѕЂрЇц ріарѕеріЋрїЊрІ┤рІЇ рІерѕ│рЇІрѕфрі«рѕЮ ріарІЇрЅХрЅЦрѕ│рЅйріЋ рЅБрѕѕрЅарЅх рЅаріЕрѕЇ рѕхрЅ│рѕЇрЇЅ рЅ░рЅђрѕІрЅђрѕЅріЋ рЅаріарЅЦрѕ«ріљрЅх рЅєріЋрїє рїірІю ріЦріЊрѕ│рѕЇрЇЇ!\\n\\n#Gursha\\n#SafaricomEthiopia #FurtherAheadTogether', 'рІГріљрЅарЅЦ! рЅ┤рІ▓ ріарЇЇрѕ« ріеріарѕюрѕфріФ рѕўрѕЇріГрЅх рѕѕрібрЅхрІ«рїхрІФрІЇрІФріЋ ріарѕхрЅ░рѕІрѕІрЇірЇб рІХріГрЅ░рѕГ ріарЅЦрІГріЋ рѕђрїѕрѕГ рІЇрѕхрїЦ рѕєрію рЅБрѕѕрѕўрІ░рїѕрЇї рЅарЅ░рѕ░рѕЮрЅХріЏрѕЇрЇб', 'рЅарѕ│рЇІрѕфрі«рѕЮ рѕЮрЅ╣ рІерІхрѕЮрїй рЇБ рІерЇЁрѕЂрЇЇ рѕўрѕЇрІЋріГрЅх ріЦріЊ рЇѕрїБріЋ 4G рІерібріЋрЅ░рѕГріћрЅх рІхрѕЁрѕе ріГрЇЇрІФ рїЦрЅЁрѕјрЅй рІФрѕѕрѕЃрѕ│рЅЦ ріЦріЋрѕхрѕФ рЇц ріЋрїЇрІ│рЅйріЋріЋ ріЦріЊрѕ│рІхрїЇ!\\n\\nрѕѕріарїѕрѕЇрїЇрѕјрЅ▒ рѕѕрѕўрѕўрІЮрїѕрЅЦ рІерѕ│рЇІрѕфрі«рѕЮріЋ рІхрѕЁрѕе - рїѕрЇЁ ріЦріЋрїјрЅЦріЮрЇБ рІѕрІ░ 0700 755 755 or 0700 700 755 ріЦріЋрІ░рІЇрѕЇ рІѕрІГрѕЮ рІѕрІ░ enterprisesupport@safaricom.et рібрѕюрІГрѕЇ ріЦріЋрѕІріГрЇб\\n\\n#SafaricomBusiness #FurtherAheadTogether', 'News Alert Рђ╝№ИЈ рІерѕђрІІрѕ│ ріерЅ░рѕЏ ріЦріЊ ріаріФрЅБрЅбрІІ рЇђрїЦрЅ│ рЅарѕўріерѕІріерІФ ріЦріЊ рЅарЇїрІ░рѕФрѕЇ рЇЊрѕірѕх ріЦрІЮ рѕЦрѕГ ріЦріЋрІ░рѕєріљ рЅ░рІЇрѕхріЌрѕЇ рІерѕџрѕЇ ріерѕўрѕИ рЅарѕўрѕ░рѕФрїерЅх рѕІрІГ рІГрїѕріЏрѕЇ рІерІџрѕЁ рІюріЊ ріЦрІЇріљрЅ│ рѕѕрѕЏрїБрѕФрЅх рѕЎріерѕФ ріарІхрѕГрїѕріЊ рІерІ░рѕерѕхріЋрЅарЅх ріеріЦрІЇріљрЅх рІерѕФрЅђ рѕўрѕєріЉріЋ рѕѕрѕўрѕерІ│рЅх рЅйрѕѕріЊрѕЇрЇб рЅарІџрѕЁ рїЅрІ│рІГ рѕІрІГ рѕўрѕерїЃрІЇ ріарѕѕріЮ рІерѕЮрЅхрѕЅ рѕўрѕерїЃ рЅа рѕІрІГ рѕўрѕІріГ рЅхрЅйрѕІрѕІрЅйрѕЂрЇб'], 'Human_Label': ['healthcare &pharma', 'Telecom', 'Media', 'Telecom', 'Media']}\n",
      "\n",
      "Validation Dataset:\n",
      "{'text': ['ADVERTISMENT ріарЇірІФ рѕЎрѕђрѕўрІх ріерЇЇрЅ░ріЏ рІерЅБрѕЁрѕЇ рѕЋріГрѕЮріЊ ріЦріЊ рІўрѕўріЊрІі рІерІІрїЇрѕЮрЅх ріарїѕрѕЇрїЇрѕјрЅх рІерѕЮріЋрѕ░рїБрЅИрІЇ рІерЅБрѕЁрѕЇ рѕЁріГрѕЮріЊрІјрЅй Ръб рѕѕрІЇрїГріЊ рѕѕрІЇрѕхрїЦ ріфріЋрЅ│рѕ«рЅх Ръб рѕѕрѕЏрІхрІФрЅх Ръб рѕѕрѕ▒рі│рѕГ рЅарѕйрЅ│ РъбрѕѕрїЅрЅарЅх(рѕѕрІѕрЇЇ рЅарѕйрЅ│) РъбрѕѕрїерїјрѕФ рѕЁрѕўрѕЮ РъбрѕѕрѕхрЇѕрЅ░рІѕрѕ▓рЅЦ РъбрѕѕрІ░рѕЮ рїЇрЇірЅх РъбрѕѕріарѕхрѕЮ рІѕрІГрѕЮ рѕ│рІГріљрѕх РъбрѕѕрѕџрїЦрѕЇ рЅарѕ║рЅ│ Ръб рѕѕріЦрѕфрѕЁріЊ рЅЂрѕГрїЦрѕЏрЅх РъбрѕѕрѕФрѕх рѕЁрѕўрѕЮ (рѕЏрІГрїЇрѕфріЋ) РъбрѕѕрЅ║рЇї ріЊ рѕѕрїГрѕГрЅх РъбрѕѕрЅІрЅЂрЅ╗ріЊ рЇјрѕерЇјрѕГ РъбрѕѕріЦрїбріЊ рѕѕріЦрЅБрїГ РъбрѕѕрІѕрїѕрЅЦ рѕЁрѕўрѕЮ РъбрѕѕрѕўріФріЋріљрЅх рѕѕрІѕрІхрѕЮ рѕѕрѕ┤рЅхрѕЮ Ръбрѕѕрїєрѕ«ріЊ рѕѕріарІГріЋ рѕЁрѕўрѕЮ РъбрѕѕрѕєрІх рѕЁрѕўрѕЮ РъбрІўрѕўріЊрІі рІерІІрїЇрѕЮрЅх ріарїѕрѕЇрїЇрѕјрЅх рЅарЅ░рЅІрѕЏрЅйріЋ ріЦріЋрѕ░рїБрѕѕріЋрЇб ­ЪЉЅріерібрЅхрІ«рїхрІФ рЅБрѕЁрѕІрІі рѕЁріГрѕЮріЊ ріарІІрЅѓрІјрЅй рѕЏрѕЁрЅарѕГ рЅарІўрѕГрЇЅ рѕЁрїІрІі рІерЅБрѕЁрѕЇ рѕЁріГрѕЮріЊ рЇЇрЅЃрІх рІФрѕѕріЋ ріљріЋрЇб ріарІхрѕФрѕ╗:ріарІ▓рѕх ріарЅарЅБ ріарІерѕГ рїцріЊ рѕхрѕЇріГ рЅЂрїЦрѕГ ­ЪЊ▓0927506650 ­ЪЊ▓0987133734 ­ЪЊ▓0939605455 рЅ┤рѕїрїЇрѕФрѕЮ рЅ╗ріЊрѕІрЅйріЋ ntvE5NmM0', 'ADVERTISMENT рѕўрѕЇріФрѕЮ ріљрїѕрѕГ ріерѕЅрѕ▓ рІеріарїЦріЋрЅхріЊ рІерѕўрїѕрїБрїарѕџрІФ рЅђрІХ рѕЋріГрѕЮріЊ рѕЏріЦріерѕЇ рѕѕріарїЦріЋрЅхріЊ рѕѕрѕўрїѕрїБрїарѕџрІФ рЅйрїЇрѕ«рЅйріЊ рѕЁрѕўрѕърЅй ріарІГріљрЅ░ріЏ рѕўрЇЇрЅхрѕё рІерѕЮріЋрѕ░рїБрЅИрІЇ рІерѕЁріГрѕЮріЊ ріарїѕрѕЇрїЇрѕјрЅХрЅй РюћрѕЏріЋріЏрІЇрѕЮ рѕхрЅЦрѕФрЅхріЊ рІЇрѕЇрЅЃрЅх рѕЋріГрѕЮріЊ РюћрІерїЅрѕЇрЅарЅхріЊ рІ│рѕї рѕўрїѕрїБрїарѕџрІФ рЅйрїЇрѕГ РюћрІЇрѕхрЅЦрѕхрЅЦ ріерЅБрІх рѕхрЅЦрѕФрЅХрЅйріЊ рІЇрѕЇрЅЃрЅХрЅй РюћрІерїАріЋрЅ╗ріЊ рІерїЁрѕЏрЅх рїЅрІ│рЅх рѕЁрѕўрѕърЅйріЋ рѕЏріерѕЮ РюћрІерѕЋрЇЃріЊрЅх ріЦріЊ ріарІІрЅѓрІјрЅй ріЦрїЁріЊ ріЦрїЇрѕГ рѕўрїБрѕўрѕЮ рѕЏрѕхрЅ░ріФріерѕЇ РюћрѕхрЅЦрѕФрЅХрЅйріЋріЊ рІЇрѕЇрЅЃрЅХрЅйріЋ рІФрѕѕ рЅђрІХ рѕЋріГрѕЮріЊ рѕЏріерѕЮ РюћрЅарЅђрІХ рѕЋріГрѕЮріЊ рІерїѕрЅА рІерЅ░рѕѕрІФрІЕ рЅЦрѕерЅХрЅйріЋ рѕЏрІЇрїБрЅх РюћрІерЇЋрѕІрѕхрЅ▓ріГ рѕ░рѕГрїђрѕф рѕЋріГрѕЮріЊ РюћрЅарІ│рѕїріЊ рІ│рѕї рїѕріЋрІ│ рѕхрЅЦрѕФрЅХрЅйріЋ рЅарЇЇрѕјрѕ«рѕхрі«рЇњ рІерЅ│рїѕрІў рѕЋріГрѕЮріЊ рѕЏрІхрѕерїЇ ­ЪЊїрѕЇрѕЮрІхріЊ рЅЦрЅЃрЅх рЅБрѕІрЅИрІЇ рѕхрЇћрѕ╗рѕірѕхрЅх ріЦріЊ рѕ░рЅЦ рѕхрЇћрѕ╗рѕірѕхрЅх рѕђріфрѕърЅй ріЦрїЁрїЇ рІўрѕўріЊрІі рЅарѕєріЉ рІерѕЁріГрѕЮріЊ рѕўрѕ│рѕфрІФрІѕрЅй рЅарѕўрЅ│рїѕрІЮ рѕЂрѕЅріЋрѕЮ рІерѕЁріГрѕЮріЊ ріарїѕрѕЇрїЇрѕјрЅХрЅй рЅарЅ░рѕўрїБрїБріЮ рІІрїІ рІФрїѕріЏрѕЅРђ╝ ріарІхрѕФрѕ╗ : ріарІерѕГ рїцріЊ рїЁрѕЏрЅарѕГ рЇќрѕірѕх рїБрЅбрІФ рїјріЋ ­ЪЊ▓ 0913468103 0953912229', 'ADVERTISMENT рѕ░рѕѕрѕъріЋ рїїрЅ│рЅИрІЇ рІерЅ░рѕўрѕ░ріерѕерѕѕрЅх рІерѕѓрѕ│рЅЦ ріарІІрЅѓріЊ рІеріЋрїЇрІх ріарѕЏріФрѕф рІхрѕГрїЁрЅх рІерѕЮріЋрѕ░рїБрЅИрІЇ ріарїѕрѕЇрїЇрѕјрЅХрЅй Рюћ рѕѕрІхрѕГрїЁрЅхрІј рЅаIFRS рІерЅ│рїѕрІў рІўрѕўріЊрІі рІерѕѓрѕ│рЅЦ ріарІФрІФрІЮ рѕхрѕГріБрЅхріЋ рѕўрІўрѕГрїІрЅх РюћрІерѕѓрѕ│рЅЦ рѕўрІЮрїѕрЅЦ ріарІФрІФрІЮ рѕхрѕЇрїаріЊ рѕўрѕхрїарЅх Рюћ рЅарѕЏріЋріЏрІЇрѕЮ рІерЅбрІЮріљрѕх рїЅрІ│рІГ рІерѕЏрѕЏріерѕГ ріарїѕрѕЇрїЇрѕјрЅх РюћрѕѕрѕЂрѕЅрѕЮ рІўрѕГрЇЇ рІерЇЋрѕ«рїђріГрЅх рЅбрІЮріљрѕх рЇЋрѕІріЋ рѕЏрІўрїІрїђрЅхріЊ рѕхрѕѕ рѕхрѕФрІЇ рЅарІЮрѕГрІЮрѕГ рѕЏрѕЏріерѕГ Рюћ рЅарЇњріцрЅйрЅхрѕф ріаріФрІЇріЋрЅ▓рїЇ рѕхрѕЇрїаріЊ рЅарїЇрѕЇрѕЮ рѕєріљ рЅарЅАрІхріЋ рѕўрѕхрїарЅх ріерЅЦрІЎ ріарїѕрѕЇрїЇрѕјрЅХрЅ╗рЅйріЋ рІГрїарЅђрѕ│рѕЅрЇбрЅаріарїѕрѕЇрїЇрѕјрЅХрЅ╗рЅйріЋ рІГрѕеріФрѕЅрЇб ріарІхрѕФрѕ╗:РђћрІ░рѕ┤ ріерЅ░рѕЏ рѕЮріЋрЅхрІІрЅЦ ріарІ│рѕФрѕй рЇірЅх рѕѕрЇірЅх ріфрѕФрІГ рЅцрЅХрЅй рѕЁріЋрЇЃ 2ріЏ рЇјрЅЁ рЅбрѕ« рЅЂрїЦрѕГ S 6 рѕхрѕЇріГ:Рђћ ­ЪЊ▓ 0911839966 0913018399 0908001616 рѕ░рѕѕрѕъріЋ рїїрЅ│рЅИрІЇ рІерЅ░рѕўрѕ░ріерѕерѕѕрЅх рІерѕѓрѕ│рЅЦ ріарІІрЅѓріЊ рІеріЋрїЇрІх ріарѕЏріФрѕф рІхрѕГрїЁрЅх', 'Samsung A04 рЅарѕўрїЇрІЏрЅх рѕЁрІГрІѕрЅ│рЅйріЋріЋ ріЦріЊрІўрѕЮріЋ! рІЏрѕгрІЇріЉ ріарЅЁрѕФрЅбрІФрЅйріЋ рІѕрІ│рѕѕ рІерѕ│рЇІрѕфрі«рѕЮ рѕ▒рЅЁ рїјрѕФ рЅарѕЏрѕѕрЅх рІерїЇрѕІрЅйріЋ ріЦріЊрІхрѕГрїѕрІЅрЇб\\n\\n#SafaricomEthiopia \\n#FurtherAheadTogether', 'ріЦріЋрі│ріЋ рІ░рѕх ріарѕѕрѕЁ рЇІрѕѓрѕЮ!\\xa0\\nрЇІрѕѓрѕЮ рІџрІФрІх ріерІхрѕгрІ│рІІ рЅаріЦрїБ рЅЂрїЦрѕГ S026160222\\xa0 \\u200b\\u200bрІерЅ░рѕеріГ рЅарїЅрѕГрѕ╗ рѕърЅ░рѕГрѕ│рІГріГрѕЇ ріарѕИріЊрЇі рѕєріЌрѕЇрЇб\\n\\nрІерѕ│рЇІрѕфрі«рѕЮ ріарїѕрѕЇрїЇрѕјрЅХрЅйріЋ рЅарѕўрїарЅђрѕЮ рЇБ рЅђрѕфрІјрЅ╣ріЋ рѕйрѕЇрѕЏрЅХрЅй ріЦріЊрѕИріЋрЇЇ!\\n\\n#FurtherAheadTogether'], 'Human_Label': ['healthcare &pharma', 'healthcare &pharma', 'financial service', 'Telecom', 'Telecom']}\n",
      "\n",
      "Test Dataset:\n",
      "{'text': ['ADVERTISMENT ріарЇірІФ рѕЎрѕђрѕўрІх ріерЇЇрЅ░ріЏ рІерЅБрѕЁрѕЇ рѕЋріГрѕЮріЊ ріЦріЊ рІўрѕўріЊрІі рІерІІрїЇрѕЮрЅх ріарїѕрѕЇрїЇрѕјрЅх рІерѕЮріЋрѕ░рїБрЅИрІЇ рІерЅБрѕЁрѕЇ рѕЁріГрѕЮріЊрІјрЅй Ръб рѕѕрІЇрїГріЊ рѕѕрІЇрѕхрїЦ ріфріЋрЅ│рѕ«рЅх Ръб рѕѕрѕЏрІхрІФрЅх Ръб рѕѕрѕ▒рі│рѕГ рЅарѕйрЅ│ РъбрѕѕрїЅрЅарЅх(рѕѕрІѕрЇЇ рЅарѕйрЅ│) РъбрѕѕрїерїјрѕФ рѕЁрѕўрѕЮ РъбрѕѕрѕхрЇѕрЅ░рІѕрѕ▓рЅЦ РъбрѕѕрІ░рѕЮ рїЇрЇірЅх РъбрѕѕріарѕхрѕЮ рІѕрІГрѕЮ рѕ│рІГріљрѕх РъбрѕѕрѕџрїЦрѕЇ рЅарѕ║рЅ│ Ръб рѕѕріЦрѕфрѕЁріЊ рЅЂрѕГрїЦрѕЏрЅх РъбрѕѕрѕФрѕх рѕЁрѕўрѕЮ (рѕЏрІГрїЇрѕфріЋ) РъбрѕѕрЅ║рЇї ріЊ рѕѕрїГрѕГрЅх РъбрѕѕрЅІрЅЂрЅ╗ріЊ рЇјрѕерЇјрѕГ РъбрѕѕріЦрїбріЊ рѕѕріЦрЅБрїГ РъбрѕѕрІѕрїѕрЅЦ рѕЁрѕўрѕЮ РъбрѕѕрѕўріФріЋріљрЅх рѕѕрІѕрІхрѕЮ рѕѕрѕ┤рЅхрѕЮ Ръбрѕѕрїєрѕ«ріЊ рѕѕріарІГріЋ рѕЁрѕўрѕЮ РъбрѕѕрѕєрІх рѕЁрѕўрѕЮ РъбрІўрѕўріЊрІі рІерІІрїЇрѕЮрЅх ріарїѕрѕЇрїЇрѕјрЅх рЅарЅ░рЅІрѕЏрЅйріЋ ріЦріЋрѕ░рїБрѕѕріЋрЇб ­ЪЉЅріерібрЅхрІ«рїхрІФ рЅБрѕЁрѕІрІі рѕЁріГрѕЮріЊ ріарІІрЅѓрІјрЅй рѕЏрѕЁрЅарѕГ рЅарІўрѕГрЇЅ рѕЁрїІрІі рІерЅБрѕЁрѕЇ рѕЁріГрѕЮріЊ рЇЇрЅЃрІх рІФрѕѕріЋ ріљріЋрЇб ріарІхрѕФрѕ╗:ріарІ▓рѕх ріарЅарЅБ ріарІерѕГ рїцріЊ рѕхрѕЇріГ рЅЂрїЦрѕГ ­ЪЊ▓0927506650 ­ЪЊ▓0987133734 ­ЪЊ▓0939605455 рЅ┤рѕїрїЇрѕФрѕЮ рЅ╗ріЊрѕІрЅйріЋ ntvE5NmM0', 'Bulk SMS рѕѕрѕЂрѕЅрѕЮ рЅбрІЮріљрѕХрЅ╗рЅйріЋ рІерѕџрѕўрЅ╣ рїЦрЅЁрѕјрЅй рІГрІъ рѕўрЅирѕЇрЇб ріерѕЂрѕѕрЅ▒ ріарѕЏрѕФрї«рЅй рЅарѕўрѕЮрѕерїЦ рІЏрѕгрІЇріЉ рѕѕрІ░ріЋрЅаріърЅ╗рЅйріЋ ріЦріЋрІ░рІЇрѕЇрЇц рЅбрІЮріљрѕ│рЅйріЋріЋ ріЦріЊрїБрїАрЇЇрЇб\\n\\nрѕѕріарїѕрѕЇрїЇрѕјрЅ▒ рѕѕрѕўрѕўрІЮрїѕрЅЦ рІхрѕЁрѕе - рїѕрї╗рЅйріЋріЋ рІГрїјрЅЦріЎрЇБ рІѕрІ░ 0700 755 755 or  0700 700 755 рІГрІ░рІЇрѕЅ рІѕрІГрѕЮ рІѕрІ░ enterprisesupport@safaricom.et рібрѕюрІГрѕЇ рІГрѕІріЕ\\n\\n#SafaricomBusiness #FurtherAheadTogether', 'Ргє№ИЈРгє№ИЈрѕхрѕѕ рІ░рЅЦрѕе рѕЏрѕГрЅєрѕх рІЕріњрЅерѕГрѕ▓рЅ▓ ріарїГрѕГ рѕўрѕерїЃ! рІЕріњрЅерѕГрѕ▓рЅ▓рІЇ рІЇрѕхрїЦ ріерѕџрїѕріЎ рѕ░рІјрЅй рІФрїѕріўрѕЂрЅх рѕўрѕерїЃ: ріаріЋрІх рІерІЕріњрЅерѕГрѕ▓рЅ▓рІЇ рІерЅ░рѕЏрѕфрІјрЅй рѕЁрЅЦрѕерЅх ріарЅБрѕЇ рІерѕєріљ рЅ░рѕЏрѕф рЅхріЊріЋрЅх рїарІІрЅх 4 рѕ░ріарЅх ріаріФрЅБрЅб рЅарІ░рѕерѕ░рЅарЅх рІхрЅЦрІ░рЅБ рѕЁрІГрІѕрЅ▒ ріарѕЇрЇЈрѕЇрЇб рІерїЇрїГрЅ▒ рѕўріљрѕ╗ ріЦрѕхріФрѕЂріЋ ріарѕЇрЅ│рІѕрЅђрѕЮрЇБ рїЇріЋ рѕЪрЅ╣ рѕ▓рѕерЅЦрѕ╣ рІеріљрЅарѕЕ рЅ░рѕЏрѕфрІјрЅйріЋ рЅфрІхрІ« рѕ▓рЅђрѕГрЇЁ ріљрЅарѕГ рѕ▓рЅБрѕЇ рѕ░рѕЮрЅ░ріЊрѕЇрЇб рѕЮріЊрѕЇрЅБрЅх рЇђрЅА рЅарІџрѕЁ рІерЅ░ріљрѕ│ рѕірѕєріЋ рІГрЅйрѕІрѕЇрЇб ріарѕЂріЋ рѕІрІГ рЅхрѕЮрѕЁрѕГрЅх рѕЎрѕЅ рѕѕрѕЎрѕЅ рЅ░рЅІрѕГрїДрѕЇрЇб рЅхріЊріЋрЅх рѕЁрІГрІѕрЅ▒ рІФрѕѕрЇѕрІЇ рЅ░рѕЏрѕф рІерЅхрїЇрѕФрІГ рЅ░рІѕрѕІрїЁ ріљрІЇрЇб рЅарІџрѕЁ рѕЮріГріЋрІФрЅх рЅхріЊріЋрЅх рѕЮрѕйрЅх рІерЅхрїЇрѕФрІГ рЅ░рѕЏрѕфрІјрЅй ріаріЋрІх ріарІ│рѕФрѕй рІЇрѕхрїЦ ріарІхрѕерІІрѕЇрЇб рІГрѕЁрѕЮ рІ░рѕЁріЋріљрЅ│рЅИрІЇріЋ рѕѕрѕўрїарЅарЅЁ рЅ│рѕхрЅд рІГрѕўрѕхрѕѕріЊрѕЇрЇб Via Elias Meseret', 'ADVERTISMENT рІѕрѕј рѕђрІГрЅЁ рЅцрЅх ріерЇѕрѕѕрїЅ рІГрІ░рІЇрѕЅ ­ЪЉЄ рІХ/рѕГ ріарЅЦрІ▒рЇБ рІХ/рѕГ рѕЎрѕђрѕўрІх ріЦріЊ рїЊрІ░ріърЅ╗рЅИрІЇ рѕфрѕЇрѕхрЅ┤рЅх рІерѕЏрѕЇрѕЏрЅхріЊ рІерѕЏріерЇІрЇѕрѕЇ рѕЁ/рѕй/рѕЏ рЅарІѕрѕј рѕђрІГрЅЁ ріерЅ░рѕЏ рІеріарЇќрѕГрЅ│рѕЏ ріЦрїБ рѕйрІФрїГ рїђрѕЮрѕ»рѕЇрЇАрЇА РЉа рІерЅдрЅ│рІЇ рѕўрїѕріЏ рѕђрІГрЅЁ ріерЅ░рѕЏ ріерЇњрІФрѕ│ 250рѕю рІѕрІ░ рѕђрІГрЅЂ ріарЅЁрїБрїФ ріарІ▓рѕ▒ ріарѕхрЇЊрѕЇрЅх рІ│рѕГ РЉА рІерЅдрЅ│рІЇ ріарїѕрѕЇрїЇрѕјрЅх рЅЁрІГрїЦ РЉб ріаріЋрІх ріЦрїБ 60 ріФрѕю рІерѕўріќрѕфрІФ ріарЇЊрѕГрЅ│рѕЏ ріЦріЊ 10 ріФрѕг рІерѕ▒рЅЁ рЅдрЅ│ ріерЇЊрѕГріфріЋрїЇ рїІрѕГ РЉБ рїЇріЋрЅБрЅ│ рЅарїІрѕФ рІѕрїф ріљрІЇрЇАрЇА РЉц рЅдрЅ│рІЇ рЅарѕфрѕЇрѕхрЅ┤рЅ▒ рЅБрѕѕрЅцрЅхріљрЅх рІерЅ░рѕўрІўрїѕрЅа ріљрІЇрЇб рѕѕрЅарѕѕрїа рѕўрѕерїЃ:Рђћ 0937411111 0938411111 рЅ│рѕЏріЮріљрЅх рѕўрїѕрѕѕрїФрЅйріЋ ріљрІЇрЇб рІХ/рѕГ ріарЅЦрІ▒рЇБ рІХ/рѕГ рѕЎрѕђрѕўрІх ріЦріЊ рїЊрІ░ріърЅ╗рЅИрІЇ рѕфрѕЇрѕхрЅ┤рЅх рІерѕЏрѕЇрѕЏрЅхріЊ рІерѕЏріерЇІрЇѕрѕЇ рѕЁ/рѕй/рѕЏ', 'ADVERTISMENT рѕ░рѕІрѕЮ­ЪЉІ ріЦріЋрі│ріЋ рІѕрІ░ WriterРђЎs Desk Consultancy рЅарѕ░рѕІрѕЮ рѕўрїАРђ╝рѕхрѕФрЅйріЋ рѕЏрѕЏріерѕГ ріљрІЇрЇбрѕѕрІЇрїцрЅх рІерѕџрІФрЅарЅЃ рѕЮріГрѕГ ріЦріЋрѕ░рїБрѕѕріЋРђ╝ РЉа рЅфрІЏ рѕЏрѕЏріерѕГ ріарїѕрѕЇрїЇрѕјрЅх РюћріЦріЋрІ░ ріФріЊрІ│рЇБ ріарІЇрѕхрЅхрѕФрѕірІФрЇБ рїЃрЇЊріЋрЇБ рЅ╗рІГріЊ ріЦріЊ рѕїрѕјрЅй рѕІрѕЅ рЅ│рІІрЅѓ рѕўрІ│рѕерѕ╗рІјрЅй рІерѕхрѕФ ріЦріЊ рІерїЅрЅЦріЮрЅх рЅфрІЏ ріЦріЋрІ▓рІФрїѕріЎ рѕЏрѕЏріерѕГ РюћріарѕхрЇѕрѕІрїі рѕ░ріљрІХрЅйріЋ ріерѕЏрІўрїІрїђрЅх рїђрѕЮрѕ« рѕѕрібріЋрЅ░рѕГрЅфрІЇ ріЦрѕхріГрѕЏрѕ░рѕЇрїаріЋ ріЦріЊ ріерЇЇрЅ░ріЏ рІерЅфрІЏ рѕЏрѕерїІрїѕрїФ рЅ░рѕўріќрЅйріЋ рЅ░рїарЅЁрѕъ рІ░ріЋрЅаріърЅйріЋ рѕЏрІўрїІрїђрЅх РЉАрЅ░рѕЏрѕфрІјрЅй рѕЏрѕЏріерѕГ ріарїѕрѕЇрїЇрѕјрЅх РюћрЅ░рѕЏрѕфрІјрЅй рЅ░рѕхрѕЏрѕџ рІерѕєріљ рІерІЇрїф рѕђрїѕрѕГ рі«рѕїрїєрЅйріЋ ріЦріЋрІ▓рІФрїѕріЎ рѕЏрѕЏріГрѕГ РюћріЦріЋрІ░ IELTS рѕІрѕЅ рЇѕрЅ░ріЊрІјрЅй рЅ░рѕЏрѕфрІјрЅйріЋ рЅЦрЅЂ рѕЏрІхрѕерїЇ ріЦріЊ рЅ░рѕЏрѕфрІјрЅйріЋ рѕѕрЅфрІЏ рібріЋрЅ░рѕГрЅфрІЇ рІерѕЏрІўрїІрїђрЅх рѕхрѕФ ріЦріЋрѕ░рѕФрѕѕріЋрЇАрЇА РЉбрІерЇЁрѕЂрЇЇ ріарїѕрѕЇрїЇрѕјрЅХрЅйрЇА РюћрІГрѕЁ ріерЇЇрЅ░ріЏ рІ▓рїЇрѕф рІФрѕІрЅИрІЇ ріЦріЊ ріерЅ░рѕѕрІФрІЕ рІ▓рЇЊрѕГрЅхрѕўріЋрЅх рІерѕўрїА рѕѕріаріФрІ│рѕџріГ рЇђрѕђрЇірІјрЅй ріЦріЊ рІерѕІрЅђ рЅйрѕјрЅ│ рѕІрѕІрЅИрІЇ рІерЇѕрїарѕФ рЇђрѕЃрЇірІјрЅй ріљрІЇрЇАрЇА РюћрІхрѕГрїЁрЅ│рЅйріЋ рЅ░рѕЏрѕфрІјрЅйріЋрЇБ ріЕрЅБріЋрІФрІјрЅйріЋ рІѕрІГрѕЮ рїЇрѕѕрѕ░рЅдрЅйріЋ 24/7 ріерѕџрїѕріЎ рІеріаріФрІ│рѕџріГ рЇђрѕЃрЇірІјрЅй рїІрѕГ рІФрїѕріЊріЏрѕЇ:: рѕѕріЦрѕГрѕхрІј рІерЅ▓рѕ▓рѕх рІѕрѕерЅђрЅХрЅйрЇБ blog рЇЁрѕЂрЇјрЅйрЇБ website contentрЇБ рІѕрІГрѕЮ рѕЏріЋріЏрІЇрѕЮ рІерЇЁрѕЂрЇЇ рЇЋрѕ«рїђріГрЅХрЅй рѕІрІГрѕЮ рІФрѕЏріГрѕЕрІјрЅ│рѕЇрЇб ­ЪЉЅрѕѕрѕЏріЋріЏрІЇрѕЮ рІерЅфрІЏ ріарїѕрѕЇрїЇрѕјрЅХрЅй рІерѕЏрѕўрѕЇріерЅ╗ ріГрЇЇрІФ ріарѕѕрѕЏрѕхріГрЇѕрѕІрЅйріЋрѕЮ рѕЇрІЕ рІФрѕерїѕріЊрѕЇРђ╝ WriterРђЎs Desk Consultancy рІЏрѕгрІЇріЉ рІФрїЇріЎріЋ­ЪЉЄ рѕхрѕЇріГ рЅЂрїЦрѕГрЇАРђћ0944096500 рЅ┤рѕїрїЇрѕФрѕЮ рЅ╗ріЊрѕЇ: рѕхрѕФрЅйріЋ рѕЏрѕЏріерѕГ ріљрІЇрЇбрѕѕрІЇрїцрЅх рІерѕџрІФрЅарЅЃ рѕЮріГрѕГ ріЦріЋрѕ░рїБрѕѕріЋРђ╝'], 'Human_Label': ['healthcare &pharma', 'Telecom', 'Media', 'financial service', 'financial service']}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the 'Tvsybkzkmapab/Amharic_ad_generation' dataset\n",
    "dataset = load_dataset('Tvsybkzkmapab/Amharic_ad_generation')\n",
    "\n",
    "# Access the 'train', 'validation', and 'test' splits\n",
    "train_dataset = dataset['train']\n",
    "validation_dataset = dataset['validation']\n",
    "test_dataset = dataset['test']\n",
    "\n",
    "# Print the first few samples from each split\n",
    "print(\"\\nTraining Dataset:\")\n",
    "print(train_dataset[:5])\n",
    "\n",
    "print(\"\\nValidation Dataset:\")\n",
    "print(validation_dataset[:5])\n",
    "\n",
    "print(\"\\nTest Dataset:\")\n",
    "print(test_dataset[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenization steps using pretrained tokenizer from huggingface \n",
    "(iocuydi/llama-2-amharic-3784m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"iocuydi/llama-2-amharic-3784m\")  # Load your trained tokenizer\n",
    "\n",
    "tokenizer.model_max_length = 512  # Set maximum sequence length\n",
    "tokenizer.padding_side = \"left\"  # Set padding strategy\n",
    "tokenizer.add_eos_token = True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(prompt):\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instruction to the model and the description of the attributes for the model\n",
    "to have context and instruct it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_tokenize_prompt(data_point):\n",
    "    text = data_point[\"text\"]\n",
    "    labels = data_point[\"Human_Label\"]\n",
    "\n",
    "    # Generate prompt template\n",
    "    template = f\"\"\"Given an Amharic advertisement sentence:\n",
    "\n",
    "\"{text}\"\n",
    "\n",
    "What are the main categories (tags) associated with this advertisement?\n",
    "\n",
    "Possible categories are: ['healthcare &pharma', 'Telecom', 'Media', 'financial service', 'consumer products', 'computing device', 'Realstate', 'retail', 'training', 'Entertainment', 'software development', 'Other']\n",
    "\n",
    "Target categories: \"{labels}\"\n",
    "\"\"\"\n",
    "\n",
    "    # Tokenize the prompt\n",
    "    return tokenize(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt)\n",
    "tokenized_val_dataset = validation_dataset.map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Up LoRA\n",
    "Now, we prepare the model for fine-tuning by applying LoRA adapters to the linear layers of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,  # Conventional\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)\n",
    "# Apply the accelerator. You can comment this out to remove the accelerator.\n",
    "model = accelerator.prepare_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Training\n",
    "In this step, we start training the fine-tuned model. You can adjust the training parameters according to your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() > 1: # If more than 1 GPU\n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from datetime import datetime\n",
    "import sentencepiece as spm\n",
    "\n",
    "\n",
    "\n",
    "project = \"Mistral_Amharic-finetuning\"\n",
    "base_model_name = \"mistral\"\n",
    "run_name = base_model_name + \"-\" + project\n",
    "output_dir = \"./\" + run_name\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    args=transformers.TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        warmup_steps=5,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        max_steps=1000,\n",
    "        learning_rate=2.5e-5, # Want about 10x smaller than the Mistral learning rate\n",
    "        logging_steps=50,\n",
    "        bf16=True,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        logging_dir=\"./logs\",        # Directory for storing logs\n",
    "        save_strategy=\"steps\",       # Save the model checkpoint every logging step\n",
    "        save_steps=50,                # Save checkpoints every 50 steps\n",
    "        evaluation_strategy=\"steps\", # Evaluate the model every logging step\n",
    "        eval_steps=50,               # Evaluate and save checkpoints every 50 steps\n",
    "        do_eval=True,                # Perform evaluation at the end of training\n",
    "        report_to=\"wandb\",           # Comment this out if you don't want to use weights & baises\n",
    "        run_name=f\"{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\"          # Name of the W&B run (optional)\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
